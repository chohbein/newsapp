{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from dbconnect import insert_articles,connect_db,insert_similar_articles\n",
    "\n",
    "import dbconnect  # Import the module itself\n",
    "import importlib\n",
    "\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self):\n",
    "        # Store the data as a list of dictionaries (each row is a dictionary)\n",
    "        self.data_list = []\n",
    "\n",
    "    def append_data(self, new_data):\n",
    "        # Append each new data as a dictionary to the list\n",
    "        self.data_list.append(new_data)\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        # Convert the list of dictionaries to a DataFrame when retrieving\n",
    "        return pd.DataFrame(self.data_list)\n",
    "\n",
    "collector = DataCollector()\n",
    "\n",
    "#===================================================================================\n",
    "#   Functions\n",
    "#===================================================================================\n",
    "\n",
    "# Function to extract significant words from a single title using POS filtering\n",
    "def extract_significant_words_from_title(title,nlp):\n",
    "    doc = nlp(title.lower())\n",
    "    title_words = []\n",
    "    # Focus on nouns, proper nouns, and possibly adjectives\n",
    "    for token in doc:\n",
    "        if token.pos_ in ('NOUN', 'PROPN', 'ADJ') and not token.is_stop:\n",
    "            title_words.append(token.lemma_)\n",
    "    return title_words\n",
    "\n",
    "# Function to extract significant words from a list of titles\n",
    "def extract_significant_words(titles,nlp):\n",
    "    processed_titles = []  # List to store processed titles\n",
    "    for title in titles:\n",
    "        title_words = extract_significant_words_from_title(title,nlp)\n",
    "        processed_titles.append(' '.join(title_words))  # Join and add to processed titles\n",
    "    return processed_titles\n",
    "\n",
    "# Function to find common significant words across all titles\n",
    "def find_common_significant_words(titles, nlp):\n",
    "    processed_titles = extract_significant_words(titles, nlp)\n",
    "    \n",
    "    # Flatten list of words for all titles\n",
    "    all_words = ' '.join(processed_titles).split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    # Find words that appear in more than one title (common significant words)\n",
    "    common_words = [word for word, count in word_counts.items() if count > 1]\n",
    "    \n",
    "    # Fallback: if no common words are found, return the original processed titles\n",
    "    if not common_words:\n",
    "        print(\"No common significant words found, returning original processed titles.\")\n",
    "        return processed_titles  # Returning processed titles if no common words are found\n",
    "    \n",
    "    return common_words\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "#   News Site Scraping\n",
    "#===================================================================================\n",
    "#       Fox\n",
    "#===================================================================================\n",
    "def foxnews(collector,nlp):\n",
    "    url = 'https://www.foxnews.com/'\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    driver.find_element(By.CLASS_NAME,'js-menu-toggle').click()\n",
    "\n",
    "    # Get Sectors\n",
    "    sector_dict = {}\n",
    "    sectors = driver.find_elements(By.CLASS_NAME,'nav-title') \n",
    "    for i in sectors:\n",
    "        sector = i.find_element(By.TAG_NAME,'a').get_attribute('aria-label')\n",
    "        sector_url = i.find_element(By.TAG_NAME,'a').get_attribute('href')\n",
    "        if sector not in sector_dict:\n",
    "            sector_dict[sector] = sector_url\n",
    "        else:\n",
    "            break\n",
    "    driver.quit()\n",
    "    \n",
    "    # Collect all data in a list\n",
    "    all_data = []\n",
    "\n",
    "    # Into Sector Dicts\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "        #print(s, sector_dict[s])\n",
    "\n",
    "        # Scroll to load more articles\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        # Extract article data\n",
    "        for ele in driver.find_elements(By.XPATH, '//a[@href]'):\n",
    "            url = ele.get_attribute('href')\n",
    "            if url.count('-') <= 2 or len(ele.text) <= 2 or 'police-and-law-enforcement' in url or not url.startswith(sector_dict[s]):\n",
    "                continue\n",
    "            text = ele.text\n",
    "            keywords = extract_significant_words_from_title(text,nlp)\n",
    "            date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "            # Collect the row data in a dictionary\n",
    "            row_data = {\n",
    "                'Source': 'FOX',\n",
    "                'Section': s,\n",
    "                'Section URL': sector_dict[s],\n",
    "                'Article Title': text,\n",
    "                'Article URL': url,\n",
    "                'Keywords': keywords,\n",
    "                'Date': date\n",
    "            }\n",
    "            all_data.append(row_data)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    # Append all data at once to the DataFrame\n",
    "    for row in all_data:\n",
    "        collector.append_data(row)\n",
    "\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "#       CNN\n",
    "#===================================================================================\n",
    "def cnn(collector,nlp):\n",
    "    url = 'https://www.cnn.com/'\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "\n",
    "    driver.find_element(By.CLASS_NAME, 'header__menu-icon-svg').click()\n",
    "\n",
    "    # Get Sectors\n",
    "    sector_dict = {}\n",
    "    sectors = driver.find_elements(By.CLASS_NAME, 'subnav__section-link')\n",
    "    for i in sectors:\n",
    "        sector = i.text\n",
    "        sector_url = i.get_attribute('href')\n",
    "        if 'about' in sector.lower():\n",
    "            break\n",
    "        sector_dict[sector] = sector_url\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Collect all data in a list\n",
    "    all_data = []\n",
    "\n",
    "    # Into Sector Dicts\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "\n",
    "        # Scroll to load more articles\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        # Extract article data\n",
    "        for ele in driver.find_elements(By.XPATH, '//a[@href]'):\n",
    "            text = ele.text\n",
    "            url = ele.get_attribute('href')\n",
    "\n",
    "            # Filter out unwanted URLs and titles\n",
    "            if len(text) < 10 or url.count('-') < 3 or text.count(' ') < 2 or url in ['', ' '] or 'cnn.com/audio/podcasts' in url:\n",
    "                continue\n",
    "\n",
    "            # Extract keywords and current date\n",
    "            keywords = extract_significant_words_from_title(text,nlp)\n",
    "            date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Collect the row data in a dictionary\n",
    "            row_data = {\n",
    "                'Source': 'CNN',\n",
    "                'Section': s,\n",
    "                'Section URL': sector_dict[s],\n",
    "                'Article Title': text,\n",
    "                'Article URL': url,\n",
    "                'Keywords': keywords,\n",
    "                'Date': date\n",
    "            }\n",
    "            all_data.append(row_data)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    # Append all data at once to the DataFrame\n",
    "    for row in all_data:\n",
    "        collector.append_data(row)\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "#       WAPO\n",
    "#===================================================================================\n",
    "def get_element_with_retry(driver,by,thing):\n",
    "    i = 0\n",
    "    while i < 100:\n",
    "        try:\n",
    "            element = driver.find_element(by,thing)\n",
    "            break\n",
    "        except:\n",
    "            driver.execute_script(\"arguments[0].scrollTop += 50;\", driver)\n",
    "    return element\n",
    "    \n",
    "def wapo(collector,nlp):\n",
    "    print(\"WAPO!!!\")\n",
    "    url = 'https://www.washingtonpost.com'\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    driver.find_element(By.XPATH, '//*[@data-testid=\"sc-header-sections-menu-button\"]').click()\n",
    "    \n",
    "    sec = driver.find_element(By.ID, 'sc-sections-nav-drawer')\n",
    "    l = sec.find_elements(By.XPATH, \"//*[starts-with(@id, '/')]\")\n",
    "    \n",
    "    sector_dict = {}\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    # Collecting sector URLs\n",
    "    for i in l:\n",
    "        dropdown_trigger = i.find_element(By.TAG_NAME, 'div')\n",
    "        actions.move_to_element(dropdown_trigger).perform()  # Hover over the element to trigger the dropdown\n",
    "        \n",
    "        test = driver.find_elements(By.TAG_NAME, 'ul')\n",
    "        t = test[len(test) - 1].find_elements(By.TAG_NAME, 'li')\n",
    "        \n",
    "        for j in t:\n",
    "            try:\n",
    "                # Use a retry mechanism to handle stale elements\n",
    "                sector_url = get_element_with_retry(j, By.TAG_NAME, 'a').get_attribute('href')\n",
    "                txt = j.find_element(By.TAG_NAME, 'a').text\n",
    "                main = i.find_element(By.TAG_NAME, 'a').text.replace(\"+\", \" \")\n",
    "                subcat = f\"{main}/{txt}\"\n",
    "                sector_dict[subcat] = sector_url\n",
    "            except StaleElementReferenceException:\n",
    "                print(\"Stale element detected. Retrying...\")\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollTop += 85;\", sec)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Collect all data in a list\n",
    "    all_data = []\n",
    "    #print(sector_dict)\n",
    "    # Navigate into each sector and collect articles\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "        #print(\"~~~~~~~~~\")\n",
    "        #print(s)\n",
    "\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            # Re-fetch elements after scrolling\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, '[data-feature-id=\"homepage/story\"]')\n",
    "            print(\"Here:\",len(elements))\n",
    "            for ele in elements:\n",
    "                text = ele.text.replace(\"\\n\", '')\n",
    "                try:\n",
    "                    article_url = ele.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if len(text) < 3 or article_url.count('-') < 3 or text.count(' ') < 3 or len(article_url) < 5:\n",
    "                    continue\n",
    "                \n",
    "                keywords = extract_significant_words_from_title(text,nlp)\n",
    "                date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                row_data = {\n",
    "                    'Source': 'WAPO',\n",
    "                    'Section': s,\n",
    "                    'Section URL': sector_dict[s],\n",
    "                    'Article Title': text,\n",
    "                    'Article URL': article_url,\n",
    "                    'Keywords': keywords,\n",
    "                    'Date': date\n",
    "                }\n",
    "                #print(\"LOOKHERE: \",row_data)\n",
    "                all_data.append(row_data)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    # Append all data at once to the DataFrame\n",
    "    for row in all_data:\n",
    "        collector.append_data(row)\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "#       NYT\n",
    "#===================================================================================\n",
    "def nyt(collector,nlp):\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get('https://www.nytimes.com/')\n",
    "    \n",
    "    # Collect section URLs\n",
    "    t = driver.find_elements(By.CSS_SELECTOR, '[data-testid^=\"nav-item-\"]')\n",
    "    sector_dict = {}\n",
    "    for i in t:\n",
    "        ele = i.find_element(By.TAG_NAME, 'a')\n",
    "        url = ele.get_attribute('href')\n",
    "        text = ele.text\n",
    "        if 'nytimes.com/spotlight/' in url or text in ['', ' ', 'Games', 'Wirecutter', 'Cooking']:\n",
    "            continue\n",
    "        sector_dict[text] = url\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Collect all data in a list\n",
    "    all_data = []\n",
    "\n",
    "    # Iterate through sectors\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "\n",
    "        # Scroll to load more articles\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        try:\n",
    "            region = driver.find_element(By.CSS_SELECTOR, '[data-testid=\"asset-stream\"]')\n",
    "        except:\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "        for ele in region.find_elements(By.TAG_NAME, 'article'):\n",
    "            lnkreg = ele.find_element(By.TAG_NAME, 'a')\n",
    "            url = lnkreg.get_attribute('href')\n",
    "            text = lnkreg.text.replace('\\n', '')\n",
    "\n",
    "            # Skip unwanted content\n",
    "            stop_hls = [\n",
    "                'contact us', 'your ad choices', 'terms of service', 'terms of sale', \n",
    "                'Â© 2024 the new york times company', 'skip to main content', 'skip to site index'\n",
    "            ]\n",
    "            if 'nytimes.com/' not in url or len(text) < 3 or text.count(' ') < 3 or text.lower() in stop_hls:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                date = ele.find_element(By.XPATH, '..').find_element(By.CSS_SELECTOR, '[data-testid=\"todays-date\"]').text\n",
    "            except:\n",
    "                date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # If date not found, use current date\n",
    "\n",
    "            print(date, text, url)\n",
    "            \n",
    "            # Extract keywords and collect data\n",
    "            keywords = extract_significant_words_from_title(text,nlp)\n",
    "            row_data = {\n",
    "                'Source': 'NYT',\n",
    "                'Section': s,\n",
    "                'Section URL': sector_dict[s],\n",
    "                'Article Title': text,\n",
    "                'Article URL': url,\n",
    "                'Keywords': keywords,\n",
    "                'Date': date\n",
    "            }\n",
    "            all_data.append(row_data)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    # Append all data at once to the DataFrame\n",
    "    for row in all_data:\n",
    "        collector.append_data(row)\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "#   \n",
    "#===================================================================================\n",
    "\n",
    "def get_similar_articles_with_source_names(datasets, source_names,model):\n",
    "    \"\"\"\n",
    "    Find similar articles across multiple datasets with emphasis on multi-source similarity.\n",
    "    Avoid repetitions in the output and use source names instead of indices.\n",
    "    \n",
    "    Args:\n",
    "        datasets (list of DataFrames): List of DataFrames where each DataFrame contains articles from a different source.\n",
    "        source_names (list of str): List of source names corresponding to each DataFrame in datasets.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (weighted_similarity_score, [title1, title2, ...], [(index1, source1_name), (index2, source2_name), ...])\n",
    "    \"\"\"\n",
    "    # Store all headlines and their embeddings in a dictionary\n",
    "    embeddings_dict = {}\n",
    "    #print(datasets)\n",
    "    for idx, df in enumerate(datasets):\n",
    "        #print(df)\n",
    "        headline_list = df['Article Title'].tolist()\n",
    "        #print(\"HLList\")\n",
    "        #print(headline_list)\n",
    "        embeddings = model.encode(headline_list)\n",
    "        embeddings_dict[idx] = (headline_list, embeddings)\n",
    "\n",
    "    # List to store aggregated results\n",
    "    aggregated_results = []\n",
    "    processed_pairs = set()\n",
    "    used_articles = set()  # Track used articles\n",
    "\n",
    "    # Iterate over all headlines in all datasets\n",
    "    for i in range(len(datasets)):\n",
    "        headlines1, embeddings1 = embeddings_dict[i]\n",
    "        \n",
    "        for k, title1 in enumerate(headlines1):\n",
    "            if (k, source_names[i]) in used_articles:\n",
    "                continue  # Skip if already used in a high-scoring match\n",
    "\n",
    "            # Initialize variables to aggregate similarity scores\n",
    "            total_similarity_score = 0\n",
    "            num_sources_matched = 0\n",
    "            similar_titles = [title1]\n",
    "            similar_indices = [(k, source_names[i])]\n",
    "            \n",
    "            # Compare against all other datasets\n",
    "            for j in range(len(datasets)):\n",
    "                if i == j:\n",
    "                    continue  # Skip comparison with itself\n",
    "\n",
    "                headlines2, embeddings2 = embeddings_dict[j]\n",
    "                \n",
    "                # Compute cosine similarity between the current headline and all headlines in the other dataset\n",
    "                cosine_matrix = cosine_similarity([embeddings1[k]], embeddings2)[0]\n",
    "                \n",
    "                # Find the most similar headline in the other dataset\n",
    "                max_sim_index = np.argmax(cosine_matrix)\n",
    "                max_similarity = cosine_matrix[max_sim_index]\n",
    "                \n",
    "                # If similarity is above a threshold, consider it a match\n",
    "                if max_similarity >= 0.52:  # Threshold to consider as a match\n",
    "                    pair = tuple(sorted([(k, i), (max_sim_index, j)]))  # Sort to avoid ordering issues\n",
    "                    if pair in processed_pairs or (max_sim_index, source_names[j]) in used_articles:\n",
    "                        continue  # Skip if this pair has already been processed or article is already used\n",
    "                    processed_pairs.add(pair)\n",
    "\n",
    "                    total_similarity_score += max_similarity\n",
    "                    num_sources_matched += 1\n",
    "                    similar_titles.append(headlines2[max_sim_index])\n",
    "                    similar_indices.append((max_sim_index, source_names[j]))\n",
    "            \n",
    "            # Calculate weighted similarity score\n",
    "            weighted_similarity_score = total_similarity_score * (num_sources_matched / len(datasets))\n",
    "            \n",
    "            # Store result if there is at least one match (i.e., num_sources_matched > 1)\n",
    "            if num_sources_matched > 1:  # Ensure at least one other source matches\n",
    "                aggregated_results.append((weighted_similarity_score, similar_titles, similar_indices))\n",
    "                # Mark all articles in the current match as used\n",
    "                used_articles.update(similar_indices)\n",
    "    \n",
    "    # Sort results by weighted similarity score in descending order\n",
    "    aggregated_results.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "#===================================================================================\n",
    "#\n",
    "#===================================================================================\n",
    "def get_sim_article_df(similar_articles_across_sources, data, nlp):\n",
    "    similar_articles_df = pd.DataFrame(columns=['Article Headlines', 'Article URLs', 'Keywords', 'Similarity Weights'])\n",
    "    \n",
    "    for i in range(len(similar_articles_across_sources)):\n",
    "        titles = similar_articles_across_sources[i][1]\n",
    "        \n",
    "        # Extract processed titles\n",
    "        processed_titles = find_common_significant_words(titles, nlp)\n",
    "        \n",
    "        # Check if the processed titles are empty or contain only stop words\n",
    "        if not processed_titles or all(len(title.strip()) == 0 for title in processed_titles):\n",
    "            print(\"No valid content in titles after preprocessing, skipping this set of articles.\")\n",
    "            continue  # Skip if no valid content\n",
    "        \n",
    "        # Proceed with TF-IDF vectorization\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(processed_titles)\n",
    "\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.sum(axis=0).A1  # Sum across all documents for global score\n",
    "        \n",
    "        tfidf_ranking = list(zip(feature_names, tfidf_scores))\n",
    "        tfidf_ranking = sorted(tfidf_ranking, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        keywords = [word for word, score in tfidf_ranking if score > 0]\n",
    "        urls = []\n",
    "        for k in similar_articles_across_sources[i][2]:\n",
    "            urls.append(data[data['Source'] == k[1]].iloc[k[0]]['Article URL'])\n",
    "\n",
    "        similar_articles_df.loc[len(similar_articles_df)] = [titles, urls, keywords, similar_articles_across_sources[i][0]]\n",
    "    \n",
    "    return similar_articles_df\n",
    "\n",
    "def safe_join(value):\n",
    "    if isinstance(value, list):\n",
    "        return '|||'.join(value)  # Use the unique delimiter\n",
    "    elif isinstance(value, str):\n",
    "        # Already a string, return as is\n",
    "        return value\n",
    "    else:\n",
    "        # Handle other types if necessary\n",
    "        return str(value)\n",
    "\n",
    "#def main():\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "foxnews(collector,nlp)\n",
    "cnn(collector,nlp)\n",
    "wapo(collector,nlp)\n",
    "nyt(collector,nlp)\n",
    "\n",
    "data = collector.get_dataframe()\n",
    "CNN = data[data['Source'] == 'CNN']\n",
    "FOX = data[data['Source'] == 'FOX']\n",
    "WAPO = data[data['Source'] == 'WAPO']\n",
    "NYT = data[data['Source'] == 'NYT']\n",
    "\n",
    "datasets = [CNN, FOX, WAPO, NYT]\n",
    "source_names = ['CNN', 'FOX', 'WAPO', 'NYT']\n",
    "\n",
    "from datetime import date\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "\n",
    "# Fill missing dates (NaT) with the current date\n",
    "data['Date'] = data['Date'].fillna(pd.Timestamp(date.today()))\n",
    "\n",
    "# Convert to string format 'YYYY-MM-DD' for database insertion\n",
    "data['Date'] = data['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "similar_articles_across_sources = get_similar_articles_with_source_names(datasets, source_names,model)\n",
    "similar_articles_df = get_sim_article_df(similar_articles_across_sources,data,nlp)\n",
    "\n",
    "# Apply the safe_join function to the columns\n",
    "similar_articles_df['Article Headlines'] = similar_articles_df['Article Headlines'].apply(safe_join)\n",
    "similar_articles_df['Article URLs'] = similar_articles_df['Article URLs'].apply(safe_join)\n",
    "similar_articles_df['Keywords'] = similar_articles_df['Keywords'].apply(safe_join)\n",
    "\n",
    "# Reload the dbconnect module to reflect changes\n",
    "importlib.reload(dbconnect)\n",
    "\n",
    "# Takes ~4min to upload to db\n",
    "print(\"Attempting db connection...\")\n",
    "connection = dbconnect.connect_db()\n",
    "print(\"connected to db\")\n",
    "\n",
    "# Change keywords from list to comma-separated string\n",
    "data['Keywords'] = data['Keywords'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "dbconnect.insert_articles(connection, data)\n",
    "\n",
    "print('connected, inserting...')\n",
    "insert_similar_articles(connection,similar_articles_df)\n",
    "connection.close()\n",
    "print(\"Inserted to db and clossed connection!\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dbconnect)\n",
    "connection = dbconnect.connect_db()\n",
    "print(\"connected to db\")\n",
    "\n",
    "# Change keywords from list to comma-separated string\n",
    "data['Keywords'] = data['Keywords'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "dbconnect.insert_articles(connection, data)\n",
    "\n",
    "print('connected, inserting...')\n",
    "insert_similar_articles(connection,similar_articles_df)\n",
    "connection.close()\n",
    "print(\"Inserted to db and clossed connection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('articles.csv',index=False)\n",
    "similar_articles_df.to_csv('simart.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
