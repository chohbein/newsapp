{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['Source','Section','Section URL','Article Title','Article URL','Keywords','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load SpaCy's English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to extract significant words from a single title using POS filtering\n",
    "def extract_significant_words_from_title(title):\n",
    "    doc = nlp(title.lower())\n",
    "    title_words = []\n",
    "    # Focus on nouns, proper nouns, and possibly adjectives\n",
    "    for token in doc:\n",
    "        if token.pos_ in ('NOUN', 'PROPN', 'ADJ') and not token.is_stop:\n",
    "            title_words.append(token.lemma_)\n",
    "    return title_words\n",
    "\n",
    "# Function to extract significant words from a list of titles\n",
    "def extract_significant_words(titles):\n",
    "    processed_titles = []  # List to store processed titles\n",
    "    for title in titles:\n",
    "        title_words = extract_significant_words_from_title(title)\n",
    "        processed_titles.append(' '.join(title_words))  # Join and add to processed titles\n",
    "    return processed_titles\n",
    "\n",
    "# Function to find common significant words across all titles\n",
    "def find_common_significant_words(titles):\n",
    "    processed_titles = extract_significant_words(titles)\n",
    "    \n",
    "    # Flatten list of words for all titles\n",
    "    all_words = ' '.join(processed_titles).split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    # Find words that appear in more than one title (common significant words)\n",
    "    common_words = [word for word, count in word_counts.items() if count > 1]\n",
    "    \n",
    "    return common_words\n",
    "\n",
    "# Example usage\n",
    "'''\n",
    "titles =['Brazil’s Supreme Court threatens to suspend X amid ongoing Musk row', \"Brazil bans X: Outrage spreads after Elon Musk's social media platform is suspended\", 'Brazilian judge orders suspension of X in dispute with Elon Musk', 'Musk’s Starlink Defies Order to Block X in Brazil']\n",
    "\n",
    "\n",
    "# Extract significant words from each title\n",
    "processed_titles = extract_significant_words(titles)\n",
    "print(\"Processed Titles with Significant Words:\")\n",
    "for title, words in zip(titles, processed_titles):\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Significant Words: {words}\\n\")\n",
    "    print(words.split(' '))\n",
    "\n",
    "# Find common significant words across all titles\n",
    "common_words = find_common_significant_words(titles)\n",
    "print(f\"Common Significant Words Across All Titles: {common_words}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foxnews():\n",
    "    url = 'https://www.foxnews.com/'\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    driver.find_element(By.CLASS_NAME,'js-menu-toggle').click()\n",
    "\n",
    "    #   Get Sectors\n",
    "    sector_dict = {}\n",
    "    sectors = driver.find_elements(By.CLASS_NAME,'nav-title') \n",
    "    for i in sectors:\n",
    "        sector = i.find_element(By.TAG_NAME,'a').get_attribute('aria-label')\n",
    "        sector_url = i.find_element(By.TAG_NAME,'a').get_attribute('href')\n",
    "        if sector not in sector_dict:\n",
    "            sector_dict[sector] = sector_url\n",
    "        else:\n",
    "            break\n",
    "    driver.quit()\n",
    "    \n",
    "    #   Into Sector Dicts\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "        print(s,sector_dict[s])\n",
    "        #   Scroll\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "        for ele in driver.find_elements(By.XPATH,'//a[@href]'):\n",
    "            url = ele.get_attribute('href')\n",
    "            if url.count('-') <= 2 or len(ele.text) <= 2 or 'police-and-law-enforcement' in url or not url.startswith(sector_dict[s]):\n",
    "                continue\n",
    "            text = ele.text\n",
    "            #print(text,url)\n",
    "            keywords = extract_significant_words_from_title(text)\n",
    "            date = datetime.today().strftime('%Y-%m-%d')\n",
    "            data.loc[len(data)] = ['FOX',s,sector_dict[s],text,url,keywords,date]\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "foxnews()\n",
    "\n",
    "#   No dates in sections page. Either; use today's date, or open article and get from each specific one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn():\n",
    "    url = 'https://www.cnn.com/'\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "\n",
    "    driver.find_element(By.CLASS_NAME,'header__menu-icon-svg').click()\n",
    "\n",
    "    sector_dict = {}\n",
    "    sectors = driver.find_elements(By.CLASS_NAME,'subnav__section-link')\n",
    "    for i in sectors:\n",
    "        sector = i.text\n",
    "        sector_url = i.get_attribute('href')\n",
    "        if 'about' in sector.lower():\n",
    "            break\n",
    "        sector_dict[sector] = sector_url\n",
    "\n",
    "    driver.quit()\n",
    "    #   Into Sector Dicts\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "        print(s)\n",
    "        #   Scroll\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        for ele in driver.find_elements(By.XPATH,'//a[@href]'):\n",
    "            text = ele.text\n",
    "            url = ele.get_attribute('href')\n",
    "            if len(text) < 10 or url.count('-') < 3 or text.count(' ') < 2 or url in ['',' '] or 'cnn.com/audio/podcasts' in url:\n",
    "                continue\n",
    "            #print(ele.text,ele.get_attribute('href'))\n",
    "            keywords = extract_significant_words_from_title(text)\n",
    "            date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            data.loc[len(data)] = ['CNN',s,sector_dict[s],text,url,keywords,date]\n",
    "        driver.quit()\n",
    "    \n",
    "cnn()\n",
    "\n",
    "#   No dates in sections page. Either; use today's date, or open article and get from each specific one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wapo_sections():\n",
    "    url = 'https://www.washingtonpost.com'\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    driver.find_element(By.XPATH,'//*[@data-testid=\"sc-header-sections-menu-button\"]').click()\n",
    "    sec = driver.find_element(By.ID,'sc-sections-nav-drawer')\n",
    "    l = sec.find_elements(By.XPATH, \"//*[starts-with(@id, '/')]\")\n",
    "    sector_dict = {}\n",
    "    actions = ActionChains(driver)\n",
    "    for i in l:\n",
    "        dropdown_trigger = i.find_element(By.TAG_NAME,'div')\n",
    "        # Hover over the element to trigger the dropdown\n",
    "        actions.move_to_element(dropdown_trigger).perform()\n",
    "        test = driver.find_elements(By.TAG_NAME,'ul')\n",
    "        #for t in test[4].find_elements(By.TAG_NAME,'li'):\n",
    "        print('h: ',len(test))\n",
    "        t = test[len(test)-1].find_elements(By.TAG_NAME,'li')\n",
    "        for j in t:\n",
    "            url = j.find_element(By.TAG_NAME,'a').get_attribute('href')\n",
    "            txt = j.find_element(By.TAG_NAME,'a').text\n",
    "            main = i.find_element(By.TAG_NAME,'a').text.replace(\"+\",\" \")\n",
    "            subcat = main + '/' + txt\n",
    "            print(subcat,url)\n",
    "            sector_dict[subcat] = url\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].scrollTop += 85;\", sec)\n",
    "        \n",
    "    driver.quit()\n",
    "    return sector_dict\n",
    "\n",
    "def wapo(sector_dict):\n",
    "    #   Into Sector Dicts\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "        print(\"~~~~~~~~~\")\n",
    "        print(s)\n",
    "        #   Scroll\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "        try:\n",
    "            region = driver.find_element(By.CSS_SELECTOR,'[data-qa=\"main\"]')\n",
    "        except:\n",
    "            driver.quit()\n",
    "            continue\n",
    "        print(len(region.find_elements(By.CSS_SELECTOR,'[data-feature-id=\"homepage/story\"]')))\n",
    "        for ele in region.find_elements(By.CSS_SELECTOR,'[data-feature-id=\"homepage/story\"]'):\n",
    "            text = ele.text.replace(\"\\n\",'')\n",
    "            url = ele.find_element(By.TAG_NAME,'a').get_attribute('href')\n",
    "            if len(text) < 3 or url.count('-') < 3 or text.count(' ') < 3 or len(url) < 5:\n",
    "                continue\n",
    "            print(text)\n",
    "            print(url)\n",
    "            keywords = extract_significant_words_from_title(text)\n",
    "            date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            data.loc[len(data)] = ['WAPO',s,sector_dict[s],text,url,keywords,date]\n",
    "        driver.quit()\n",
    "\n",
    "sector_dict = get_wapo_sections()\n",
    "wapo(sector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       NYT\n",
    "def get_nyt():\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get('https://www.nytimes.com/')\n",
    "    t = driver.find_elements(By.CSS_SELECTOR, '[data-testid^=\"nav-item-\"]')\n",
    "\n",
    "    sector_dict = {}\n",
    "    for i in t:\n",
    "        ele = i.find_element(By.TAG_NAME,'a')\n",
    "        url = ele.get_attribute('href')\n",
    "        text = ele.text\n",
    "        if 'nytimes.com/spotlight/' in url or text in ['',' ','Games','Wirecutter','Cooking']:\n",
    "            continue\n",
    "        sector_dict[text] = url\n",
    "\n",
    "    driver.quit()\n",
    "    #   Into Sector Dicts\n",
    "    for s in sector_dict:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(sector_dict[s])\n",
    "        print(\"~~~~~~~~~\")\n",
    "        print(s)\n",
    "        for i in range(8):\n",
    "            driver.execute_script(\"window.scrollTo(0, window.pageYOffset + 700);\")\n",
    "            time.sleep(0.3)\n",
    "        try:\n",
    "            region = driver.find_element(By.CSS_SELECTOR, '[data-testid=\"asset-stream\"]')\n",
    "        except:\n",
    "            driver.quit()\n",
    "            continue\n",
    "        print('len: ',len(region.find_elements(By.TAG_NAME,'article')))\n",
    "        for ele in region.find_elements(By.TAG_NAME,'article'):\n",
    "            lnkreg = ele.find_element(By.TAG_NAME,'a')\n",
    "            url = lnkreg.get_attribute('href')\n",
    "            text = lnkreg.text.replace('\\n','')\n",
    "\n",
    "            stop_hls = ['contact us','your ad choices','terms of service','terms of sale','© 2024 the new york times company','skip to main content','skip to site index']\n",
    "            if 'nytimes.com/' not in url or len(text) < 3 or text.count(' ') < 3 or text.lower() in stop_hls:\n",
    "                continue\n",
    "\n",
    "            date = ele.find_element(By.XPATH,'..').find_element(By.CSS_SELECTOR,'[data-testid=\"todays-date\"]').text\n",
    "            print(date,text,url)\n",
    "            \n",
    "            keywords = extract_significant_words_from_title(text) \n",
    "            data.loc[len(data)] = ['NYT',s,sector_dict[s],text,url,keywords,date]\n",
    "        driver.quit()\n",
    "\n",
    "get_nyt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_similar_articles_with_source_names(datasets, source_names):\n",
    "    \"\"\"\n",
    "    Find similar articles across multiple datasets with emphasis on multi-source similarity.\n",
    "    Avoid repetitions in the output and use source names instead of indices.\n",
    "    \n",
    "    Args:\n",
    "        datasets (list of DataFrames): List of DataFrames where each DataFrame contains articles from a different source.\n",
    "        source_names (list of str): List of source names corresponding to each DataFrame in datasets.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (weighted_similarity_score, [title1, title2, ...], [(index1, source1_name), (index2, source2_name), ...])\n",
    "    \"\"\"\n",
    "    # Store all headlines and their embeddings in a dictionary\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    for idx, df in enumerate(datasets):\n",
    "        headline_list = df['Article Title'].tolist()\n",
    "        embeddings = model.encode(headline_list)\n",
    "        embeddings_dict[idx] = (headline_list, embeddings)\n",
    "\n",
    "    # List to store aggregated results\n",
    "    aggregated_results = []\n",
    "    processed_pairs = set()\n",
    "    used_articles = set()  # Track used articles\n",
    "\n",
    "    # Iterate over all headlines in all datasets\n",
    "    for i in range(len(datasets)):\n",
    "        headlines1, embeddings1 = embeddings_dict[i]\n",
    "        \n",
    "        for k, title1 in enumerate(headlines1):\n",
    "            if (k, source_names[i]) in used_articles:\n",
    "                continue  # Skip if already used in a high-scoring match\n",
    "\n",
    "            # Initialize variables to aggregate similarity scores\n",
    "            total_similarity_score = 0\n",
    "            num_sources_matched = 0\n",
    "            similar_titles = [title1]\n",
    "            similar_indices = [(k, source_names[i])]\n",
    "            \n",
    "            # Compare against all other datasets\n",
    "            for j in range(len(datasets)):\n",
    "                if i == j:\n",
    "                    continue  # Skip comparison with itself\n",
    "\n",
    "                headlines2, embeddings2 = embeddings_dict[j]\n",
    "                \n",
    "                # Compute cosine similarity between the current headline and all headlines in the other dataset\n",
    "                cosine_matrix = cosine_similarity([embeddings1[k]], embeddings2)[0]\n",
    "                \n",
    "                # Find the most similar headline in the other dataset\n",
    "                max_sim_index = np.argmax(cosine_matrix)\n",
    "                max_similarity = cosine_matrix[max_sim_index]\n",
    "                \n",
    "                # If similarity is above a threshold, consider it a match\n",
    "                if max_similarity >= 0.52:  # Threshold to consider as a match\n",
    "                    pair = tuple(sorted([(k, i), (max_sim_index, j)]))  # Sort to avoid ordering issues\n",
    "                    if pair in processed_pairs or (max_sim_index, source_names[j]) in used_articles:\n",
    "                        continue  # Skip if this pair has already been processed or article is already used\n",
    "                    processed_pairs.add(pair)\n",
    "\n",
    "                    total_similarity_score += max_similarity\n",
    "                    num_sources_matched += 1\n",
    "                    similar_titles.append(headlines2[max_sim_index])\n",
    "                    similar_indices.append((max_sim_index, source_names[j]))\n",
    "            \n",
    "            # Calculate weighted similarity score\n",
    "            weighted_similarity_score = total_similarity_score * (num_sources_matched / len(datasets))\n",
    "            \n",
    "            # Store result if there is at least one match (i.e., num_sources_matched > 1)\n",
    "            if num_sources_matched > 1:  # Ensure at least one other source matches\n",
    "                aggregated_results.append((weighted_similarity_score, similar_titles, similar_indices))\n",
    "                # Mark all articles in the current match as used\n",
    "                used_articles.update(similar_indices)\n",
    "    \n",
    "    # Sort results by weighted similarity score in descending order\n",
    "    aggregated_results.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "# Example usage:\n",
    "CNN = data[data['Source'] == 'CNN']\n",
    "FOX = data[data['Source'] == 'FOX']\n",
    "WAPO = data[data['Source'] == 'WAPO']\n",
    "NYT = data[data['Source'] == 'NYT']\n",
    "\n",
    "datasets = [CNN, FOX, WAPO, NYT]\n",
    "source_names = ['CNN', 'FOX', 'WAPO', 'NYT']\n",
    "\n",
    "similar_articles_across_sources = get_similar_articles_with_source_names(datasets, source_names)\n",
    "\n",
    "# Output results\n",
    "'''\n",
    "for score, titles, indices in similar_articles_across_sources:\n",
    "    print(f\"Weighted Score: {score:.4f}, Articles: {titles}, Indices: {indices}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   How to reverse back to 'data' to get url's and such for each similar article\n",
    "src = similar_articles_across_sources[1][2][1][1]\n",
    "i = similar_articles_across_sources[1][2][1][0]\n",
    "print(i,src)\n",
    "print(similar_articles_across_sources[1][1][1])\n",
    "data[data['Source'] == src].iloc[i]['Article URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_article_df(similar_articles_across_sources):\n",
    "    similar_articles_df = pd.DataFrame(columns=['Article Headlines','Article URLs','Keywords','Similarity Weights'])\n",
    "    for i in range(len(similar_articles_across_sources)):\n",
    "        titles = similar_articles_across_sources[i][1]\n",
    "        #print(similar_articles_across_sources[i])\n",
    "            # Extract processed titles\n",
    "        processed_titles = find_common_significant_words(titles)\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(processed_titles)\n",
    "\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.sum(axis=0).A1  # Sum across all documents for global score\n",
    "\n",
    "        tfidf_ranking = list(zip(feature_names, tfidf_scores))\n",
    "        tfidf_ranking = sorted(tfidf_ranking, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        keywords = [word for word, score in tfidf_ranking if score > 0]\n",
    "        urls = []\n",
    "        for k in similar_articles_across_sources[i][2]:\n",
    "            urls.append(data[data['Source'] == k[1]].iloc[k[0]]['Article URL'])\n",
    "\n",
    "        #print(keywords)\n",
    "        #print(urls)\n",
    "        similar_articles_df.loc[len(similar_articles_df)] = [titles,urls,keywords,similar_articles_across_sources[i][0]]\n",
    "    return similar_articles_df\n",
    "\n",
    "similar_articles_df = get_sim_article_df(similar_articles_across_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_articles_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
