{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Doc Summarizing\n",
    "In this file, we will explore different approaches to summarizing a topic in the news given multiple articles discussing the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Extract sample data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#from dbconnect import insert_articles,connect_db,insert_similar_articles\n",
    "\n",
    "#import dbconnect  # Import the module itself\n",
    "import importlib\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import time\n",
    "from rouge import Rouge\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import importlib\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import utils\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Multi-doc Summarizer\n",
    "\n",
    "\n",
    "connection = utils.connect_db()\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        sa.simart_id,\n",
    "        a.article_id,\n",
    "        atx.article_content\n",
    "\tFROM similar_articles sa\n",
    "        JOIN junct_simart_articles jsa ON jsa.simart_id = sa.simart_id\n",
    "        JOIN articles a ON a.article_id = jsa.article_id\n",
    "        JOIN article_text atx ON atx.article_id = a.article_id\n",
    "\tWHERE sa.similar_weight >= 0.8\n",
    "\tAND EXISTS (\n",
    "        SELECT 1\n",
    "        FROM articles a2 \n",
    "        JOIN junct_simart_articles jsa2 ON jsa2.article_id = a2.article_id\n",
    "        WHERE jsa2.simart_id = sa.simart_id\n",
    "        AND a2.date >= NOW() - INTERVAL '2 days'\n",
    "\t)\n",
    "\tGROUP BY sa.simart_id,a.article_id,atx.article_content\n",
    "\tORDER BY sa.simart_id;\n",
    "                       \"\"\",con=connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('simart_id')['article_content'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    cleaned = []\n",
    "    for t in texts:\n",
    "        #   Remove missing articles\n",
    "        if pd.isna(t):\n",
    "            continue\n",
    "        t_clean = re.sub(r'[^\\w\\s\\'\\\"$£€;\\-:.,]', '', str(t)) # remove excessive punctuation, keep periods, quotes\n",
    "        t_clean = t_clean.replace('  ',' ')\n",
    "        #   Empty texts\n",
    "        if t_clean.lower() == 'nan' or not t_clean.strip():\n",
    "            continue\n",
    "        \n",
    "        cleaned.append(t_clean)\n",
    "        #   check for dupe text\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    rows_to_drop = []   # Drop empty matches after cleaning\n",
    "\n",
    "    for i,r in df.iterrows():\n",
    "        text_list = r['article_content']\n",
    "        \n",
    "        #   Clean texts\n",
    "        cleaned_texts = preprocess(text_list)\n",
    "        seen = set()\n",
    "        unique_lst = [\n",
    "            x\n",
    "            for x in cleaned_texts \n",
    "            if not (x.strip().lower() in seen or seen.add(x.strip().lower()))\n",
    "        ]\n",
    " \n",
    "        df.at[i,'article_content'] = unique_lst\n",
    "        if len(cleaned_texts) < 2:\n",
    "            rows_to_drop.append(i)\n",
    "\n",
    "    df.drop(rows_to_drop,inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "grouped_df = clean(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: Centroid-based Summarization for MDS\n",
    "- Source: https://aclanthology.org/W00-0403.pdf \\\n",
    "    This study dives into an approach to multi-doc summarization for news articles specifically. \\\n",
    "    It highlights a centroid-based approach to collecting the most important sentences across all articles. \\\n",
    "    We will be replicating this study for approach 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      simart_id                                    article_content\n",
      "36         8053  [Tech billionaire Elon Musk said that the soci...\n",
      "41         8150  [Tech billionaire Elon Musk said that the soci...\n",
      "43         8185  [March's full \"Blood Worm Moon,\" a phenomenon ...\n",
      "46         8231  [The mysterious disappearance of an American c...\n",
      "47         8232  [An Indiana woman who was found alive in her c...\n",
      "...         ...                                                ...\n",
      "2731      14266  [Commissioner Richard Trumka Jr. is photograph...\n",
      "2732      14267  [NIH Director Jayanta Bhattacharya, left, and ...\n",
      "2733      14268  [Speaker of the House Rep. Mike Johnson, R-La....\n",
      "2736      14271  [VATICAN CITY AP Pope Leo XIV laid out the vis...\n",
      "2738      14277  [NUUK, Greenland AP Lisa Sólrun Christiansen g...\n",
      "\n",
      "[1413 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "def fit_vectorizer():\n",
    "    # Grab all\n",
    "    connection = utils.connect_db()\n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "        SELECT jsa.simart_id, ax.article_id, ax.article_content \n",
    "        FROM junct_simart_articles jsa\n",
    "        JOIN article_text ax ON jsa.article_id = ax.article_id\n",
    "    \"\"\", con=connection)\n",
    "    connection.close()\n",
    "\n",
    "    grouped_df = df.groupby('simart_id')['article_content'].apply(list).reset_index()\n",
    "    grouped_df = clean(grouped_df)  # Clean text\n",
    "\n",
    "    # Function to remove highly similar duplicates\n",
    "    def remove_dupes(df):\n",
    "        new_df = df.copy()\n",
    "        for i, row in new_df.iterrows():\n",
    "            txt = row['article_content']\n",
    "            to_remove = set()\n",
    "\n",
    "            for x in txt:\n",
    "                for y in txt:\n",
    "                    if x == y:\n",
    "                        continue\n",
    "                    sim = fuzz.ratio(x, y)\n",
    "                    if sim > 85:  # Near-duplicate detected\n",
    "                        shorter, longer = (x, y) if len(x) < len(y) else (y, x)\n",
    "                        to_remove.add(shorter)  # Keep the longer version\n",
    "\n",
    "            # Remove duplicates for this row\n",
    "            new_df.at[i, 'article_content'] = [t for t in txt if t not in to_remove]\n",
    "\n",
    "        return new_df\n",
    "\n",
    "    grouped_df = remove_dupes(grouped_df)\n",
    "\n",
    "    # Check for remaining duplicates\n",
    "    rows_to_drop = grouped_df[grouped_df['article_content'].apply(len) < 2].index\n",
    "    grouped_df.drop(index=rows_to_drop, inplace=True)\n",
    "\n",
    "    print(grouped_df)\n",
    "\n",
    "    article_texts = []\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        token_pattern=r'(?u)\\b[\\w-]+\\b'\n",
    "    )\n",
    "\n",
    "    for content_list in grouped_df['article_content']:\n",
    "        for article in content_list:\n",
    "            article_texts.append(article)\n",
    "\n",
    "\n",
    "    vectorizer.fit(article_texts)\n",
    "    #   Run once, save vectorizer\n",
    "    joblib.dump(vectorizer,'tfidf_vectorizer.joblib')\n",
    "\n",
    "fit_vectorizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate cluster centroid\n",
    "The centroid, as seen in the study, is the average TF-IDF vector of all articles within a cluster of articles on the same topic. \\\n",
    "Thus, $\n",
    "\\text{Centroid} = \\frac{\\sum_{i=1}^{n} \\mathbf{v}_i}{n}\n",
    "$ For $\\mathbf{v}_i$ = TF-IDF for each 1,..,n articles within a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_sentences(article):\n",
    "    return [sent.text for sent in nlp(article).sents]\n",
    "\n",
    "def generate_summary(cluster_articles, vectorizer, summary_length=3):\n",
    "    #MEAD-style summary generator for a single cluster\n",
    "    # Transform articles to TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.transform(cluster_articles)\n",
    "    \n",
    "    # Compute cluster centroid\n",
    "    centroid = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "    \n",
    "    # Score sentences\n",
    "    scored_sentences = []\n",
    "    for article in cluster_articles:\n",
    "        sentences = tokenize_sentences(article)\n",
    "        for sent in sentences:\n",
    "            sent_vec = vectorizer.transform([sent])\n",
    "            similarity = cosine_similarity(sent_vec, [centroid])[0][0]\n",
    "            scored_sentences.append({\n",
    "                'text': sent,\n",
    "                'score': similarity,\n",
    "                'position': len(scored_sentences)  # Track original order\n",
    "            })\n",
    "\n",
    "     # Remove redundant sentences\n",
    "    selected = []\n",
    "    for sent in sorted(scored_sentences, key=lambda x: -x['score']):\n",
    "        if not selected or all(\n",
    "            cosine_similarity(\n",
    "                vectorizer.transform([sent['text']]),\n",
    "                vectorizer.transform([s['text']])\n",
    "            )[0][0] < 0.7 for s in selected\n",
    "        ):\n",
    "            selected.append(sent)\n",
    "            if len(selected) >= summary_length * 2:\n",
    "                break\n",
    "    \n",
    "    # Preserve chronological order\n",
    "    final_summary = sorted(selected[:summary_length], key=lambda x: x['position'])\n",
    "    return ' '.join([s['text'] for s in final_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_summary_df = pd.DataFrame(columns=['simart_id','summary'])\n",
    "\n",
    "for i,r in grouped_df.iterrows():\n",
    "    simart_id = r['simart_id']\n",
    "    cluster_articles = r['article_content']\n",
    "    \n",
    "    summary = generate_summary(cluster_articles,vectorizer,summary_length=5)\n",
    "    \n",
    "    a1_summary_df = pd.concat([\n",
    "        a1_summary_df,\n",
    "        pd.DataFrame({'simart_id': [simart_id], 'summary': [summary]})\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Approach 2: PRIMERA (Pyramid-based Masked Sentence Pre-training)\n",
    "PRIMERA is a leading model for multi-doc summarization. \\\n",
    "It uses a pre-trained method called Entity Pyramid to identify the important sentences by getting frequency across documents and how representative each is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/PRIMERA\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/PRIMERA\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary2(model, tokenizer, article_clust):\n",
    "    # Check if the input cluster is empty\n",
    "    if not article_clust:\n",
    "        return \"No content to summarize.\"\n",
    "\n",
    "    # Use <doc-sep> to separate articles in the cluster\n",
    "    article_text = \"<doc-sep>\".join(article_clust)\n",
    "    \n",
    "    # Tokenize the input, limiting max length and truncating if necessary\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=2048, truncation=True).to(\"cuda\")\n",
    "\n",
    "    try:\n",
    "        # Generate summary with the specified parameters\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=200, min_length=50, length_penalty=2.0, num_beams=6)\n",
    "        #   length_pentalty -> Penalty set on length of summary, >1 penalizes long summaries, <1 favors\n",
    "        #   num_beams -> # of beams used in beam search (deciding over summary candidates): high = better quality, slower, less diverse\n",
    "        # Decode the summary\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generating summary: {e}\")\n",
    "        return \"Error during summarization.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 1737 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1188 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1108 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1229 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1393 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1906 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1098 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    }
   ],
   "source": [
    "a2_summary_df = pd.DataFrame(columns=['simart_id','summary'])\n",
    "\n",
    "for i,r in grouped_df.iterrows():\n",
    "    simart_id = r['simart_id']\n",
    "    cluster_articles = r['article_content']\n",
    "    \n",
    "    summary = generate_summary2(model,tokenizer,cluster_articles)\n",
    "    \n",
    "    a2_summary_df = pd.concat([\n",
    "        a2_summary_df,\n",
    "        pd.DataFrame({'simart_id': [simart_id], 'summary': [summary]})\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "To score and compare our approaches, we will use a combination of approaches:\n",
    "1. Manual comparison\n",
    "    - I will manually look at a subset of summaries and determine the best ones.\n",
    "2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "    - Combine all documents and evaluate the summary's precision and recall with ROUGE.\n",
    "    - Precision: how relevant the summarized phrases are to the crux of the sources.\n",
    "    - Recall: how much of the source material is included in the summary\n",
    "\n",
    "### Limitations\n",
    "Our method of evaluating the best approach is very limited. Without use of an LLM or large-scale human evaluation, it is hard to determine how good a summary is. For this reason, I will heavily rely on my own judgements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries(candidate, reference):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Only use ROUGE-2 for speed\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)[0]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        'rouge2_precision': scores['rouge-2']['p'],\n",
    "        'rouge2_recall': scores['rouge-2']['r'],\n",
    "        'rouge2_f1': scores['rouge-2']['f'],\n",
    "        'duration_seconds': end_time - start_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY (Approach 1):  On Monday, Newsom shared a model ordinance for cities and counties to \"immediately address dangerous and unhealthy encampments and connect people experiencing homelessness with shelter and services.\" Gov. Gavin Newsom; people at a homeless encampment in California Getty Images Newsom is also encouraging local leaders to use their authority, affirmed by the U.S. Supreme Court, to address encampments. PROGRESSIVE JOURNALIST SAYS NEWSOM MUST TAKE 'ACCOUNTABILITY' FOR HOW HE 'DESTROYED' CALIFORNIA California Gov. Gavin Newsom MediaNews GroupEast Bay Times via Getty Images Monday's announcement is in addition to the release of $3.3 billion in voter-approved Proposition 1 funding, which Newsom's office said will be made available later today to communities statewide. \"Governor Newsom is the first governor to actively address this issue in our state, and he is reversing a crisis that was decades in the making,\" Newsom's office said. Saying that there are no more excuses, California Gov. Gavin Newsom on Monday released a model ordinance for local municipalities to address homeless encampments, along with announcing new funding for treatment and housing.\n",
      "precision:  0.9814814814814815\n",
      "recall:  0.13790112749349523\n",
      "SUMMARY (Approach 2):  California Gov. Gavin Newsom released details about his multibillion-dollar plan to tackle the homeless crisis in the Golden State and is pushing cities and counties to take \"immediate action.\" \"Theres nothing compassionate about letting people die on the streets,\" Newsom said in a news release. DEM CITY USES 'BAIT-AND-SWITCH' TACTIC TO APPROVE HOMELESS SHELTER, LOCALS ERUPT \"Local leaders asked for resources we delivered the largest state investment in history. They asked for legal clarity the courts delivered. Now, were giving them a model they can put to work immediately, with urgency and with humanity, to resolve encampments and connect people to shelter, housing, and care. The time for inaction is over. . The ordinance is backed in part by $3.3 billion in new Prop 1 funding, Newsom's office announced, adding that the governor is \"calling on all local governments to act without delay.\"\n",
      "precision:  0.9861111111111112\n",
      "recall:  0.12315698178664354\n",
      "~~~~\n",
      "SUMMARY (Approach 1):  President Trump leaves Monday for the first major foreign trip of his second term, focusing on business deals in Saudi Arabia, Qatar and the United Arab Emirates as his administration struggles to broker an end to the war in Gaza. \"Mohammed bin Salman Saudi Arabia's crown prince is very likely to say that so sours the atmosphere, that that's not something that he can be engaging in at this time,\" said Ross, who's now at the Washington Institute for Near East Policy. President Trump and Saudi Crown Prince Mohammed bin Salman meet in Riyadh on May 20, 2017. President Donald Trump is embarking this week on a high-stakes tour of the Persian Gulf region, targeting business deals and strategic partnerships with three oil-rich nations: Saudi Arabia, the United Arab Emirates and Qatar. Under former President Joe Biden, U.S. relations with Gulf states cooled, particularly after Biden vowed to make Saudi Crown Prince Mohammed bin Salman a \"pariah\" over the 2018 killing of journalist Jamal Khashoggi.\n",
      "precision:  0.9803921568627451\n",
      "recall:  0.1718213058419244\n",
      "SUMMARY (Approach 2):  President Trump waves after taking questions from reporters outside the White House on May 8. President Trump leaves Monday for the first major foreign trip of his second term, focusing on business deals in Saudi Arabia, Qatar and the United Arab Emirates as his administration struggles to broker an end to the war in Gaza. In 2017, Egyptian President Abdel Fattah el-Sisi from left, Saudi King Salman and President Trump attend a ceremonial launch of the Global Center for Combating Extremist Ideology. . On this trip, he will showcase a promise by Saudi Arabia to invest $600 billion in the United States over the next four years and pledges from the UAE to spend $1.4 trillion over 10 years. \"The Saudis, the Emiratis and Qataris are going to fall all over themselves over who can outdo themselves to welcome the president,\" said Steven Cook, a senior fellow for the Middle East at the Council on Foreign Relations. \"In many ways, the trip is a replay of the inaugural\n",
      "precision:  0.9693251533742331\n",
      "recall:  0.18098510882016036\n",
      "~~~~\n",
      "SUMMARY (Approach 1):  U.S. Rep. Randy Feenstra, R-Iowa, filed paperwork on Monday to run for Iowa governor in the 2026 election. Feenstra, who was first elected to the U.S. House in 2020, filed the paperwork for \"Feenstra for Governor\" with the Iowa Ethics and Campaign Disclosure Board, which is needed to launch a gubernatorial campaign, according to the Iowa Capital Dispatch. Feenstra is the only U.S. House member from Iowa considering a run for governor. Iowa Republican Rep. Randy Feenstra filed to run for governor Monday, setting the stage for what could become a politically tense primary contest in the GOP. CBS News was first to report Feenstra's plan to join next year's Iowa gubernatorial race, which will be an open contest due to Republican Gov. Kim Reynolds' announcement last month that she would not seek another term.\n",
      "precision:  0.992\n",
      "recall:  0.20097244732576985\n",
      "SUMMARY (Approach 2):  Feenstra, who was first elected to the U.S. House in 2020, filed the paperwork for \"Feenstra for Governor\" with the Iowa Ethics and Campaign Disclosure Board, which is needed to launch a gubernatorial campaign, according to the Iowa Capital Dispatch. The congressman is seeking to replace Republican Gov. Kim Reynolds, who said last month she would not run for a third term in 2026. Feenstra has not publicly announced a campaign for governor. The GOP primary in the Hawkeye State could potentially be crowded, although former state Rep. Brad Sherman is the only Republican to have officially joined the race after he launched his campaign in February. Other potential GOP candidates are Mike Bousselot, a state senator; Iowa Secretary of Agriculture Mike Naig; state representative Bobby Kaufmann; and Iowa House Speaker Pat Grassley. . This Iowa race could be one of the most closely watched gubernatorial races in the state and nationally next year.Iowa is a training ground for operatives and\n",
      "precision:  0.9748427672955975\n",
      "recall:  0.25121555915721233\n",
      "~~~~\n",
      "SUMMARY (Approach 1):  At the time, Medicare was barred from negotiating drug prices, but that changed with the 2022 passage of the Democrats Inflation Reduction Act, which gave Medicare the historic power to bargain over prices for a small number of drugs annually. President Trump is taking aim at U.S. drug prices with an executive order geared toward forcing drug companies to match the lower prices paid in other developed countries. The U.S., by and large, doesn't set prices, so drug companies have more freedom to see what prices the market here will bear. Trump originally signed an executive order on most favored nation pricing in late 2020 during his first term that would have tied Medicare Part B drug prices to those in other countries. The health departments top leaders will be meeting with drug company executives over the next 30 days to offer new prices on drugs that are based off what other countries pay, Oz said on Monday.\n",
      "precision:  0.9868421052631579\n",
      "recall:  0.055350553505535055\n",
      "SUMMARY (Approach 2):  President Donald Trump announced Sunday that he plans to resurrect a controversial policy from his first term that aims to reduce drug costs by basing payments for certain medicines on their prices in other countries. His prior rule, called Most Favored Nation, was finalized in late 2020 but blocked by federal courts and rescinded by then-President Joe Biden in 2021. . However, it is unclear what payments or drugs the new directive would apply to. In a Truth Social post Sunday evening, Trump said he will be signing an executive order Monday morning that he argues would drastically lower drug prices. I will be instituting a MOST FAVORED NATIONS POLICY whereby the United States will pay the same price as the Nation that pays the lowest price anywhere in the World. The directive comes as the Trump administration is also looking to impose tariffs on pharmaceutical imports, which had been exempted from such levies enacted during the presidents first term. The tariffs could exacerbate shortages of certain drugs, particularly generic medicines, and eventually\n",
      "precision:  0.9761904761904762\n",
      "recall:  0.06051660516605166\n",
      "~~~~\n",
      "SUMMARY (Approach 1):  , Israel Hamas is expected to release Israeli-American hostage Edan Alexander on Monday, as part of what the militant group said was a step toward reaching a ceasefire agreement with Israel. The statement said Israel has not agreed to a ceasefire in exchange for Alexander's release. , Israel Hamas says it will release Israeli American Edan Alexander from Gaza as a step toward reaching a ceasefire agreement with Israel, according to a statement on the militant group's Telegram channel. Hamas has announced the imminent release of Edan Alexander, the last known living American hostage held in Gaza. HAMAS CLAIMS IT WILL RELEASE AMERICAN HOSTAGE EDAN ALEXANDER Edan Alexander thanks\n",
      "precision:  0.9468085106382979\n",
      "recall:  0.026807228915662652\n",
      "SUMMARY (Approach 2):  A woman holds a poster bearing a portrait of an Israeli hostage held by Hamas in Gaza since October 2023 next to several portaits, including two C of Israeli-US captive Edan Alexander, at Hostages Square in Tel Aviv as people wait for his release on May 12, 2025. TEL AVIV, Israel Hamas is expected to release an Israeli-American hostage on Monday, as part of what the militant group said was a step toward reaching a ceasefire agreement with Israel. Alexander, a 21 year-old Israeli soldier raised in New Jersey, is the last remaining U.S. citizen captured by Hamas on Oct. 7, 2023 and held in Gaza, who is believed to still be alive. The bodies of four other Americans are still held in the Gaza, according to the US. President Trump posted on his social media platform that the release is a \"step taken in good faith\" to put an end to the war. In a statement, Hamas said it had been holding\n",
      "precision:  0.98125\n",
      "recall:  0.047289156626506026\n",
      "~~~~\n"
     ]
    }
   ],
   "source": [
    "lim = 0\n",
    "for i, r in grouped_df.iterrows():\n",
    "    lim += 1\n",
    "    if lim > 5:\n",
    "        break\n",
    "    id = r['simart_id']\n",
    "    \n",
    "    summary_1 = a1_summary_df[a1_summary_df['simart_id'] == id]['summary'].iloc[0]\n",
    "    summary_2 = a2_summary_df[a2_summary_df['simart_id'] == id]['summary'].iloc[0]\n",
    "    \n",
    "    #   Combine documents and evaluate summary\n",
    "    combined_articles = \" [DOC_SEP] \".join(r['article_content'])\n",
    "    eval = evaluate_summaries(summary_1,combined_articles)\n",
    "    eval2 = evaluate_summaries(summary_2,combined_articles)\n",
    "\n",
    "    print(\"SUMMARY (Approach 1): \", summary_1)\n",
    "    print('precision: ',eval['rouge2_precision'])\n",
    "    print('recall: ',eval['rouge2_recall'])\n",
    "    #for x in r['article_content']:\n",
    "    #    print(x)\n",
    "    \n",
    "    print(\"SUMMARY (Approach 2): \", summary_2)\n",
    "    print('precision: ',eval2['rouge2_precision'])\n",
    "    print('recall: ',eval2['rouge2_recall'])\n",
    "    #for x in r['article_content']:\n",
    "    #    print(x)\n",
    "    \n",
    "    print(\"~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdict\n",
    "Both summaries are very similar in what they extract from the articles. \\\n",
    "They also share very similar precisions and recalls, but as we stated earlier, this is not a good determining factor. \\\n",
    "However, PRIMERA surpasses approach 1 by creating a more coherent flow of sentence structure. \\\n",
    "While approach 1 is decent at capturing similar meaning as PRIMERA, its sentence structure is very choppy, redundant, and messy. \\ \n",
    "Lastly, approach 1 runs much faster. Time is not a big concern for our use though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
