{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#from dbconnect import insert_articles,connect_db,insert_similar_articles\n",
    "\n",
    "#import dbconnect  # Import the module itself\n",
    "import importlib\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrape\n",
    "import pandas as pd\n",
    "class DataCollector:\n",
    "    def __init__(self):\n",
    "        self.data_list = []\n",
    "\n",
    "    def append_data(self, new_data):\n",
    "        self.data_list.append(new_data)\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return pd.DataFrame(self.data_list)\n",
    "    \n",
    "collector = DataCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape.wapo(collector)\n",
    "df = collector.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reader(driver):\n",
    "    body = ' '.join([p.text for p in driver.find_elements(By.XPATH, '//*[@data-component-name=\"paragraph\"]') if p])\n",
    "    return body\n",
    "\n",
    "def fox_reader(driver):\n",
    "    ctx = driver.find_element(By.CLASS_NAME,'article-body')\n",
    "    body_eles = [p for p in ctx.find_elements(By.TAG_NAME,'p')]\n",
    "    if body_eles[0].get_attribute('class') != \"speakable\":\n",
    "        body_eles.pop(0)\n",
    "    body = ' '.join([p.text for p in body_eles])\n",
    "    return body\n",
    "\n",
    "def ap_reader(driver):\n",
    "    ctx = driver.find_element(By.TAG_NAME,'bsp-story-page')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body\n",
    "\n",
    "def npr_reader(driver):\n",
    "    ctx = driver.find_element(By.ID,'storytext')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body\n",
    "\n",
    "def hufpo_reader(driver):\n",
    "    ctx = driver.find_element(By.ID,'entry-body')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body\n",
    "\n",
    "\n",
    "def cbs_reader(driver):\n",
    "    ctx = driver.find_element(By.CLASS_NAME,'content__body')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Get text from articles\n",
    "def read_articles(url_list):\n",
    "    options = Options()\n",
    "    options.add_argument('-headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.fullscreen_window()\n",
    "\n",
    "    source_map = {\n",
    "        'CNN': cnn_reader,\n",
    "        'FOX': fox_reader,\n",
    "        'AP': ap_reader,\n",
    "        'NPR': npr_reader,\n",
    "        'HuffPost': hufpo_reader,\n",
    "        'CBS': cbs_reader\n",
    "    }\n",
    "\n",
    "    for url,source in url_list:\n",
    "        print(\"going \",url)\n",
    "        driver.get(url)\n",
    "\n",
    "        reader_function = source_map[source]\n",
    "        body = reader_function(driver)\n",
    "\n",
    "        print(body)\n",
    "        print(\"~~~~~~\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "test = [\n",
    "    ['https://www.cnn.com/2025/02/09/business/trump-tariffs-steel-aluminum/index.html','CNN'],\n",
    "    ['https://www.cbsnews.com/news/trump-to-announce-tariffs-aluminum-steel-imports-monday/','CBS'],\n",
    "    ['https://www.foxnews.com/us/colombian-nationals-arrested-connection-dozens-home-burglaries-oregon','FOX'],\n",
    "    ['https://apnews.com/article/eric-adams-indictment-109ef48bd49bc8adc1850709c99bf666','AP'],\n",
    "    ['https://www.npr.org/2025/02/11/nx-s1-5292505/einstein-ring-euclid','NPR'],\n",
    "    ['https://www.huffpost.com/entry/nancy-gertner-donald-trump-move_n_67ab19ace4b025b52f75e895','HuffPost']\n",
    "        ]\n",
    "\n",
    "    \n",
    "read_articles(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   get_full_articles.py\n",
    "#       Read full article text and insert to database.\n",
    "#       DO NOT CALL 'run_all'. This runs for the entirety of the database.\n",
    "#\n",
    "#\n",
    "#       Questions: \n",
    "#           Read every article, or every article with matches (simart_id's)?\n",
    "#               Every article. In summarization, create 2 different approaches to 1 article vs matching articles.\n",
    "#\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import importlib\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import utils\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self):\n",
    "        self.data_list = []\n",
    "\n",
    "    def append_data(self, new_data):\n",
    "        self.data_list.append(new_data)\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return pd.DataFrame(self.data_list)\n",
    "\n",
    "#   Send the text corresponding to the article_id to database.\n",
    "def insert_articles(connection, df):\n",
    "    cursor = connection.cursor()\n",
    "    for index, row in df.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO article_text (article_id,article_content)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (article_id) DO NOTHING\n",
    "        \"\"\", (row['article_id'], row['article_content']))\n",
    "    \n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "\n",
    "def cnn_reader(driver):\n",
    "    body = ' '.join([p.text for p in driver.find_elements(By.XPATH, '//*[@data-component-name=\"paragraph\"]') if p])\n",
    "    return body\n",
    "\n",
    "def fox_reader(driver):\n",
    "    ctx = driver.find_element(By.CLASS_NAME,'article-body')\n",
    "    body_eles = [p for p in ctx.find_elements(By.TAG_NAME,'p')]\n",
    "    if body_eles[0].get_attribute('class') != \"speakable\":\n",
    "        body_eles.pop(0)\n",
    "    body = ' '.join([p.text for p in body_eles])\n",
    "    return body\n",
    "\n",
    "def ap_reader(driver):\n",
    "    try:\n",
    "        driver.find_element(By.ID,'onesignal-slidedown-cancel-button').click()\n",
    "        driver.find_element(By.CSS_SELECTOR,'.bcpNotificationBarClose.bcpNotificationBarCloseIcon.bcpNotificationBarCloseTopRight').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        ctx = driver.find_element(By.TAG_NAME,'bsp-story-page')\n",
    "    except:\n",
    "        ctx = driver.find_element(By.CSS_SELECTOR,'.Page-content')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body\n",
    "\n",
    "def npr_reader(driver):\n",
    "    ctx = driver.find_element(By.ID,'storytext')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body\n",
    "\n",
    "def hufpo_reader(driver):\n",
    "    ctx = driver.find_element(By.ID,'entry-body')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body\n",
    "\n",
    "\n",
    "def cbs_reader(driver):\n",
    "    ctx = driver.find_element(By.CLASS_NAME,'content__body')\n",
    "    body = ' '.join([p.text for p in ctx.find_elements(By.TAG_NAME,'p')])\n",
    "    return body\n",
    "\n",
    "#   Get text from articles\n",
    "def read_articles(url_list):\n",
    "    options = Options()\n",
    "    options.add_argument('-headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.fullscreen_window()\n",
    "\n",
    "    source_map = {\n",
    "        'CNN': cnn_reader,\n",
    "        'FOX': fox_reader,\n",
    "        'AP': ap_reader,\n",
    "        'NPR': npr_reader,\n",
    "        'HuffPost': hufpo_reader,\n",
    "        'CBS': cbs_reader\n",
    "    }\n",
    "    ready_df = pd.DataFrame(columns=['article_id','article_content'])\n",
    "    for url,article_id,source in url_list:\n",
    "        #   Paywalled sources\n",
    "        if source in ['NYT','WAPO']:\n",
    "            body = np.NAN\n",
    "            ready_df.loc[len(ready_df)] = [article_id,body]\n",
    "            continue\n",
    "        #print(source,url)\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            reader_function = source_map[source]\n",
    "            body = reader_function(driver)\n",
    "        except:\n",
    "            body = np.NAN\n",
    "\n",
    "        ready_df.loc[len(ready_df)] = [article_id,body]\n",
    "\n",
    "    driver.quit()\n",
    "    return ready_df\n",
    "\n",
    "#   DO NOT CALL.\n",
    "#       Runs on all articles in database.\n",
    "#       For more selective calls, get urls and send to read_articles\n",
    "def run_all():\n",
    "    connection = utils.connect_db()\n",
    "\n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "        SELECT a.article_id,a.url,a.article_source FROM articles a\n",
    "        JOIN junct_simart_articles jsa ON jsa.article_id = a.article_id\n",
    "        JOIN similar_articles sa ON jsa.simart_id = sa.simart_id\n",
    "    \"\"\", con=connection)\n",
    "    connection.close()\n",
    "\n",
    "    print(df)\n",
    "    url_list = df[['url','article_source']].values.tolist()\n",
    "    res = read_articles(url_list)\n",
    "    \n",
    "def run_daily():\n",
    "    connection = utils.connect_db()\n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "\tsa.simart_id,\n",
    "\ta.article_id,\n",
    "\ta.url,\n",
    "\ta.date,\n",
    "\ta.article_source\n",
    "\tFROM similar_articles sa\n",
    "\tJOIN junct_simart_articles jsa ON jsa.simart_id = sa.simart_id\n",
    "\tJOIN articles a ON a.article_id = jsa.article_id\n",
    "\tJOIN junct_simart_keywords jsk ON jsk.simart_id = sa.simart_id\n",
    "\tJOIN keywords kw ON kw.keyword_id = jsk.keyword_id\n",
    "\tWHERE sa.similar_weight >= 0.8\n",
    "\tAND EXISTS (\n",
    "\tSELECT 1\n",
    "\tFROM articles a2 \n",
    "\tJOIN junct_simart_articles jsa2 ON jsa2.article_id = a2.article_id\n",
    "\tWHERE jsa2.simart_id = sa.simart_id\n",
    "\tAND a2.date >= NOW() - INTERVAL '2 days'\n",
    "\t)\n",
    "\tGROUP BY sa.simart_id, sa.similar_weight, a.article_id, a.title, a.url, a.date, a.article_section, a.section_url, a.article_source, a.image, a.subheading\n",
    "\tORDER BY sa.simart_id;\n",
    "                           \"\"\",con=connection)\n",
    "\n",
    "    url_list = df[['url','article_id','article_source']].values.tolist()\n",
    "    res = read_articles(url_list)\n",
    "    res['article_id'] = res['article_id'].astype(int)\n",
    "\n",
    "    insert_articles(connection,res)\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "run_daily()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Multi-doc Summarizer\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import importlib\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import utils\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "connection = utils.connect_db()\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        sa.simart_id,\n",
    "        a.article_id,\n",
    "        atx.article_content\n",
    "\tFROM similar_articles sa\n",
    "        JOIN junct_simart_articles jsa ON jsa.simart_id = sa.simart_id\n",
    "        JOIN articles a ON a.article_id = jsa.article_id\n",
    "        JOIN article_text atx ON atx.article_id = a.article_id\n",
    "\tWHERE sa.similar_weight >= 0.8\n",
    "\tAND EXISTS (\n",
    "        SELECT 1\n",
    "        FROM articles a2 \n",
    "        JOIN junct_simart_articles jsa2 ON jsa2.article_id = a2.article_id\n",
    "        WHERE jsa2.simart_id = sa.simart_id\n",
    "        AND a2.date >= NOW() - INTERVAL '2 days'\n",
    "\t)\n",
    "\tGROUP BY sa.simart_id,a.article_id,atx.article_content\n",
    "\tORDER BY sa.simart_id;\n",
    "                       \"\"\",con=connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simart_id</th>\n",
       "      <th>article_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [simart_id, article_content]\n",
       "Index: []"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = df.groupby('simart_id')['article_content'].apply(list).reset_index()\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    cleaned = []\n",
    "    for t in texts:\n",
    "        #   Remove missing articles\n",
    "        if pd.isna(t):\n",
    "            continue\n",
    "        t_clean = re.sub(r'[^\\w\\s\\'$£€;\\-:.]', '', str(t)) # remove excessive punctuation, keep periods\n",
    "        t_clean = t_clean.replace('  ',' ')\n",
    "        #   Empty texts\n",
    "        if t_clean.lower() == 'nan' or not t_clean.strip():\n",
    "            continue\n",
    "        \n",
    "        cleaned.append(t_clean)\n",
    "        #   check for dupe text\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    rows_to_drop = []   # Drop empty matches after cleaning\n",
    "\n",
    "    for i,r in df.iterrows():\n",
    "        text_list = r['article_content']\n",
    "        \n",
    "        #   Clean texts\n",
    "        cleaned_texts = preprocess(text_list)\n",
    "        seen = set()\n",
    "        unique_lst = [\n",
    "            x\n",
    "            for x in cleaned_texts \n",
    "            if not (x.strip().lower().replace(\"'\", \"\").replace('\"', \"\").replace(\"’\", \"\") in seen or seen.add(x.strip().lower().replace(\"'\", \"\").replace('\"', \"\").replace(\"’\", \"\")))\n",
    "        ]\n",
    " \n",
    "        df.at[i,'article_content'] = unique_lst\n",
    "        if len(cleaned_texts) < 2:\n",
    "            rows_to_drop.append(i)\n",
    "\n",
    "    df.drop(rows_to_drop,inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "grouped_df = clean(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: TF-IDF vectorization\n",
    "- Source: https://aclanthology.org/W00-0403.pdf \\\n",
    "    This study dives into an approach to multi-doc summarization for news articles specifically. \\\n",
    "    It highlights a centroid-based approach to collecting the most important sentences across all articles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "def fit_vectorizer():\n",
    "    # Grab all\n",
    "    connection = utils.connect_db()\n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "        SELECT jsa.simart_id, ax.article_id, ax.article_content \n",
    "        FROM junct_simart_articles jsa\n",
    "        JOIN article_text ax ON jsa.article_id = ax.article_id\n",
    "    \"\"\", con=connection)\n",
    "    connection.close()\n",
    "\n",
    "    grouped_df = df.groupby('simart_id')['article_content'].apply(list).reset_index()\n",
    "    grouped_df = clean(grouped_df)  # Clean text\n",
    "\n",
    "    # Function to remove highly similar duplicates\n",
    "    def remove_dupes(df):\n",
    "        new_df = df.copy()\n",
    "        for i, row in new_df.iterrows():\n",
    "            txt = row['article_content']\n",
    "            to_remove = set()\n",
    "\n",
    "            for x in txt:\n",
    "                for y in txt:\n",
    "                    if x == y:\n",
    "                        continue\n",
    "                    sim = fuzz.ratio(x, y)\n",
    "                    if sim > 85:  # Near-duplicate detected\n",
    "                        shorter, longer = (x, y) if len(x) < len(y) else (y, x)\n",
    "                        to_remove.add(shorter)  # Keep the longer version\n",
    "\n",
    "            # Remove duplicates for this row\n",
    "            new_df.at[i, 'article_content'] = [t for t in txt if t not in to_remove]\n",
    "\n",
    "        return new_df\n",
    "\n",
    "    grouped_df = remove_dupes(grouped_df)\n",
    "\n",
    "    # Check for remaining duplicates\n",
    "    rows_to_drop = grouped_df[grouped_df['article_content'].apply(len) < 2].index\n",
    "    grouped_df.drop(index=rows_to_drop, inplace=True)\n",
    "\n",
    "    print(grouped_df)\n",
    "\n",
    "    article_texts = []\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        token_pattern=r'(?u)\\b[\\w-]+\\b'\n",
    "    )\n",
    "\n",
    "    for content_list in grouped_df['article_content']:\n",
    "        for article in content_list:\n",
    "            article_texts.append(article)\n",
    "\n",
    "\n",
    "    vectorizer.fit(article_texts)\n",
    "    #   Run once, save vectorizer\n",
    "    joblib.dump(vectorizer,'tfidf_vectorizer.joblib')\n",
    "\n",
    "fit_vectorizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate cluster centroid\n",
    "The centroid, as seen in the study, is the average TF-IDF vector of all articles within a cluster of articles on the same topic. \\\n",
    "Thus, $\n",
    "\\text{Centroid} = \\frac{\\sum_{i=1}^{n} \\mathbf{v}_i}{n}\n",
    "$ For $\\mathbf{v}_i$ = TF-IDF for each 1,..,n articles within a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in grouped_df.iterrows():\n",
    "    group = r['article_content']\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
